{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RS-stql-8yW"
   },
   "source": [
    "# Universidad del Rosario\n",
    "# Procesamiento de Lenguaje Natural\n",
    "# Caso de estudio 2 corte 2\n",
    "# Fabián Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i33H7jq8K0E"
   },
   "source": [
    "# Caso corte 2\n",
    "\n",
    "- Para entregar antes del día martes 18 de abril a las 18:00. En grupos de dos integrantes\n",
    "\n",
    "- Cargar el .pynb en la carpeta de las aulas virtuales\n",
    "\n",
    "- Los avances del caso se realizará el día 12 de abril en clase y se evaluarán los avances y el trabajo en la clase.\n",
    "\n",
    "- El trabajo completo se cargará de acuerdo a las anteriores instrucciones.\n",
    "\n",
    "Considere el siguiente conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "myJMUhL98Rlw"
   },
   "outputs": [],
   "source": [
    "url= \"https://raw.githubusercontent.com/Fabian830348/Bases_Datos/master/base_LDA.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es utilizado para español\n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "# Este es utilizado oara ingles\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snZ7_Ywk8E6i"
   },
   "source": [
    "# PUNTO 1\n",
    "\n",
    "* Considere el siguiente conjunto de documentos ***obtener los temas (Topics) de una serie de artículos (noticias)*** que está etiquetados con un tema (el corpus tiene 20 temas).\n",
    "\n",
    "- El fichero en formato .Json contiene una serie de documento en los que le asigna una temática.\n",
    "\n",
    "- Cada elemento del Json contiene:\n",
    "\n",
    "    - **content**: Contenido del artículo\n",
    "    - **target**: Identificador del target\n",
    "    - **target_names**: Nombre del target\n",
    "\n",
    "\n",
    "* ***NOTA***: *Ya que vamos a realizar un ejercicio de aprendizjae no supervisado, no debemos tener en cuentas los temas en los que alguien (un humano experto) ha clasificado estos textos. La idea del ejercicio es obtener los temas distintos de los que hablan los artículos (***Clusterizar artículos***). En este sentido es útil saber a priori el número de temas distintos que puede tener, pero en ningún caso el target de los artículos entraria al algoritmo de aprendizaje*\n",
    "\n",
    "* Para realizar este ejercicio se seguirán los siguientes pasos\n",
    "\n",
    "2. Exploración de los datos y entendimiento del conjunto de datos\n",
    "3. Interprete el siguiente código\n",
    "\n",
    "```\n",
    "df.groupby(['target', 'target_names']).count()\n",
    "```\n",
    "4. Para la Normalización utilizar ***spaCy*** y realizar las siguientes acciones en una sola función\n",
    "\n",
    "    4.1. Pasar las frases a minúsculas.\n",
    "\n",
    "    4.2. Eliminar los signos de puntuación.\n",
    "\n",
    "    4.3. Eliminar las palabras con menos de 3 carácteres.\n",
    "\n",
    "    4.4. Eliminar las palabras con mas de 12 carácteres.\n",
    "\n",
    "    4.5. Eliminar las Stop-Words.\n",
    "\n",
    "    4.6. Eliminar los emails\n",
    "\n",
    "    4.7. Eliminar los saltos de línea\n",
    "\n",
    "    4.8. Eliminar las comillas simples\n",
    "\n",
    "    4.9. Filtrar las palabras que no sean Sustantivo, Adjetivo, Verbo o Adverbio.\n",
    "\n",
    "    4.10. Pasar la palabra a su lema\n",
    "\n",
    "5. Crear del diccionario y la bolsa de palabras\n",
    "\n",
    "    5.1 Corpus tokenizado: \"*documents_tok*\"\n",
    "\n",
    "    5.2 Diccionario: \"*dictionary*\"\n",
    "\n",
    "    5.3 Corpus: \"*corpus*' que es la bolsa de palabras de gensim\n",
    "\n",
    "6. Creacción del modelo\n",
    "\n",
    "    6.1 Crear un modelo con 20 temas\n",
    "\n",
    "    6.2 Utilice la instrucción \"chunksize=100\". ¿Qué efecto tiene?\n",
    "\n",
    "7. Visualización. \n",
    "\n",
    "    7.1 Visualizar los resultados con la librería de *pyLDAvis*\n",
    "\n",
    "    7.2 ¿Qué puede concluir de la visualización?\n",
    "\n",
    "8. ¿Qué sucede si seleccionó 12, 15, 17 temas? Haga la visualización en estos casos. ¿Cuál es mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Cargas Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: markm@bigfoot.sps.mot.com (Mark Monninger)\\nSubject: Re: Auto air conditioning without Freon\\nNntp-Posting-Host: 223.250.10.7\\nReply-To: rapw20@email.sps.mot.com\\nOrganization: SPS\\nDistribution: usa\\nLines: 20\\n\\nIn article <1993Apr15.222600.11690@research.nj.nec.com>  \\nbehanna@syl.nj.nec.com (Chris BeHanna) writes:\\n>  ...\\n> \\tSeveral chemists already have come up with several substitutes for\\n> R12.  You don't hear about them because the Mobile Air Conditioning  \\nSociety\\n> (MACS), that is, the people who stand to rake in that $300 to $1000 per\\n> retrofit per automobile, have mounted an organized campaign to squash  \\nthose\\n> R12 substitutes out of existence if not ban them altogether (on very  \\nshaky\\n> technical grounds, at best, on outright lies at worst).\\n>  ...\\n\\nNow, I'm not saying you're wrong because I know that the R-12 substitutes  \\nexist, but this sounds a lot like the 200mpg carbs that the oil companies  \\nkeep us all from getting.\\n\\nMark\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(url)\n",
    "df.content.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El siguiente codigo nos agrupa por la etiqueta y nos dice cuantas noticias hay por cada una de las etiquetas realizando un conteo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>rec.autos</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>sci.med</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>sci.space</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 content\n",
       "target target_names                     \n",
       "0      alt.atheism                    92\n",
       "1      comp.graphics                 120\n",
       "2      comp.os.ms-windows.misc       122\n",
       "3      comp.sys.ibm.pc.hardware      138\n",
       "4      comp.sys.mac.hardware         122\n",
       "5      comp.windows.x                135\n",
       "6      misc.forsale                  145\n",
       "7      rec.autos                     137\n",
       "8      rec.motorcycles               118\n",
       "9      rec.sport.baseball            125\n",
       "10     rec.sport.hockey              140\n",
       "11     sci.crypt                     152\n",
       "12     sci.electronics               128\n",
       "13     sci.med                       124\n",
       "14     sci.space                     153\n",
       "15     soc.religion.christian        137\n",
       "16     talk.politics.guns            105\n",
       "17     talk.politics.mideast         129\n",
       "18     talk.politics.misc             91\n",
       "19     talk.religion.misc             87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Este codigo nos indica cuantas noticias hay en cada una de las etiquetas.\n",
    "df.groupby(['target', 'target_names']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Normalización Columna Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    #Identificar el idioma\n",
    "    idioma = detect(text)\n",
    "    #pasar a minuscula\n",
    "    text = text.lower()\n",
    "    #Eliminar correos electronicos  \\S*@\\S*\\s\n",
    "    #\\S Identifica caracteres que no son espacio\n",
    "    #* Lo realiza continuamente\n",
    "    #\\s Identifica espacios\n",
    "    text = re.sub(\"\\S*@\\S*\\s\",\"\",text)\n",
    "    #print(text)\n",
    "    #Eliminar saltos de linea, comillas simples, y signos de puntuación\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\",\" \", text) \n",
    "    #print(text)\n",
    "    # Tokenización\n",
    "    if idioma == \"en\":\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_es(text)\n",
    "    #Realizamos la tokenización dependiendo del idioma\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    \n",
    "    #StopWord\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    print(stopwords)\n",
    "    #Eliminar las palabras con caracteres menos a 3 y mayores a 12\n",
    "    tokens_c= [palabras for palabras in tokens if (len(palabras)<12 and len(palabras)> 3)and (palabras.lower() not in  stopwords)]\n",
    "    print(tokens_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'else', 'two', 'well', 'anyhow', 'besides', 'nevertheless', '’s', 'per', 'through', 'wherein', 'n‘t', 'unless', 'neither', 'with', 'last', \"'d\", 'many', 'much', 'whereafter', 'became', \"'re\", 'often', 'take', 'n’t', 'fifty', 'off', 'former', 'hereafter', 'him', 'whether', 'into', 'once', 'of', 'over', 'mine', 'could', 'three', 'five', 'eight', 'make', 'never', 'whose', 'quite', 'front', 'that', 'done', 'everywhere', 'on', 'very', 'ourselves', 'every', 'whenever', 'someone', 'doing', 'his', 'mostly', 'it', 'either', 'himself', 'both', 'towards', 'side', 'what', '’m', 'would', 'should', '’re', '‘d', 'latterly', 'ca', '‘ll', 'is', 'top', 'why', 'give', 'some', 'above', 'say', \"'ve\", 'hereby', 'whole', 'hereupon', 'rather', 'all', 'several', 'herein', 'except', 'hundred', 'because', 'amount', 'afterwards', 'these', 'none', 'perhaps', 'yet', '’ll', 'at', \"n't\", 'serious', 'by', '‘ve', 'which', 'but', 'go', 'therein', 'forty', 'sixty', 'meanwhile', 'just', 'does', 'had', 'sometime', 'whom', 'least', 'again', 'too', 'were', 'nobody', 'whereupon', 'they', 'another', 'elsewhere', 'sometimes', 'without', 'thereby', 'part', 'formerly', 'under', 'moreover', 'now', '‘m', 'due', 'thence', 'how', 'until', 'seem', 'my', 'anyone', 'next', 'always', 're', 'anyway', 'otherwise', 'becoming', 'cannot', 'enough', 'together', 'although', 'ours', 'bottom', 'where', 'itself', 'therefore', 'are', 'own', 'beforehand', 'the', 'everything', 'thus', 'along', 'nor', 'move', 'not', 'her', 'must', 'seemed', 'see', 'during', 'only', 'used', 'keep', 'few', 'then', 'against', 'to', 'than', 'same', 'an', 'whither', \"'s\", 'fifteen', 'further', 'you', 'almost', 'so', 'somehow', 'though', 'please', 'ten', 'who', 'around', 'via', 'seems', 'put', 'might', 'others', 'after', 'call', 'your', '‘s', 'from', 'been', 'whence', 'me', 'beyond', 'yours', 'there', 'namely', 'throughout', 'while', 'nowhere', 'most', 'this', 'full', 'amongst', 'becomes', 'whoever', 'them', 'its', 'other', 'third', 'anything', 'may', 'anywhere', '‘re', 'and', 'myself', 'yourselves', 'since', 'various', 'across', 'six', 'indeed', 'themselves', 'show', \"'ll\", 'in', 'when', 'here', 'empty', 'will', 'twenty', 'herself', 'before', 'our', '’ve', 'hence', 'any', 'more', 'about', 'less', 'down', 'among', 'for', 'already', 'something', 'do', 'somewhere', 'whatever', 'really', 'back', 'latter', 'wherever', 'thereupon', 'out', 'whereby', 'also', 'being', \"'m\", 'within', 'hers', 'toward', 'be', 'or', 'us', 'as', 'made', 'i', 'she', 'onto', 'get', 'below', 'nine', 'did', 'regarding', 'each', 'can', 'whereas', 'no', 'even', 'nothing', 'if', 'was', 'still', 'has', 'behind', 'thereafter', 'using', 'first', 'become', '’d', 'eleven', 'their', 'a', 'one', 'such', 'name', 'twelve', 'four', 'yourself', 'he', 'ever', 'we', 'those', 'beside', 'between', 'seeming', 'upon', 'noone', 'however', 'alone', 'have', 'thru', 'everyone', 'am', 'up'}\n",
      "['mark', 'monninger', 'subject', 'auto', 'freon', 'nntp', 'posting', 'host', 'reply', 'lines', 'article', 'chris', 'behanna', 'writes', 'chemists', 'come', 'substitutes', 'hear', 'mobile', 'society', 'macs', 'people', 'stand', 'rake', '1000', 'retrofit', 'automobile', 'mounted', 'organized', 'campaign', 'squash', 'substitutes', 'existence', 'altogether', 'shaky', 'technical', 'grounds', 'best', 'outright', 'lies', 'worst', 'saying', 'wrong', 'know', 'substitutes', 'exist', 'sounds', 'like', '200mpg', 'carbs', 'companies', 'getting', 'mark']\n"
     ]
    }
   ],
   "source": [
    "a = \"From: markm@bigfoot.sps.mot.com (Mark Monninger)\\nSubject: Re: Auto air conditioning without Freon\\nNntp-Posting-Host: 223.250.10.7\\nReply-To: rapw20@email.sps.mot.com\\nOrganization: SPS\\nDistribution: usa\\nLines: 20\\n\\nIn article <1993Apr15.222600.11690@research.nj.nec.com>  \\nbehanna@syl.nj.nec.com (Chris BeHanna) writes:\\n>  ...\\n> \\tSeveral chemists already have come up with several substitutes for\\n> R12.  You don't hear about them because the Mobile Air Conditioning  \\nSociety\\n> (MACS), that is, the people who stand to rake in that $300 to $1000 per\\n> retrofit per automobile, have mounted an organized campaign to squash  \\nthose\\n> R12 substitutes out of existence if not ban them altogether (on very  \\nshaky\\n> technical grounds, at best, on outright lies at worst).\\n>  ...\\n\\nNow, I'm not saying you're wrong because I know that the R-12 substitutes  \\nexist, but this sounds a lot like the 200mpg carbs that the oil companies  \\nkeep us all from getting.\\n\\nMark\\n\\n\"\n",
    "preprocessing(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW_B7GpQ9_2B"
   },
   "source": [
    "# PUNTO 2\n",
    "\n",
    "- Con los datos normalizados y vectorizados aplique el método LSI. Compare los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAMyKIMv-Jjg"
   },
   "source": [
    "# PUNTO 3\n",
    "\n",
    "- Con los datos normalizados y vectorizados. ¿Es posible aplicar k-means?\n",
    "\n",
    "- En caso de una respuesta afirmativa. Utilice alguno el método del codo, el método de la silueta, y el método GAP para determinar cuáles valores de $k$ (número de clusters) son sugeridos y aplique el modelo.\n",
    "\n",
    "- Comparar resultados con los métodos anteriores: k-means, LDA y LSI.\n",
    "\n",
    "- Concluya"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
