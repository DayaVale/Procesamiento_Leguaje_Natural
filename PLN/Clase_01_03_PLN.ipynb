{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzJgBR3zx2Eu"
      },
      "source": [
        "# Generalidades de Spacy\n",
        "\n",
        "- Es una librería de python para Procesamiento de Lenguaje Natural.\n",
        "\n",
        "- Una ventaja respesto a NLTK es que trae modelos preentrenados para diferentes idiomas, entre ellos, el español.\n",
        "\n",
        "- Tiene mejor rendimiento que NTLK.\n",
        "\n",
        "- Algunas tareas que se pueden hacer con  Spacy:\n",
        "\n",
        "    - Tokenización\n",
        "\n",
        "    - Stop Words\n",
        "\n",
        "    - Part of Speech (PoS)\n",
        "\n",
        "    - Lematización\n",
        "\n",
        "- Para poder usar los modelos preentrenados de Spacy se deben descargar:\n",
        "\n",
        "     - es_core_news_md\n",
        "\n",
        "     - es_core_news_sm\n",
        "\n",
        "El primero más pesado que el segundo.\n",
        "\n",
        "- Para más información se puede consultar \n",
        "[Spacy](\"https://spacy.io/\")-\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b6O-QXv8z4Ym"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.1.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.24.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.28.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\dayan\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cDMYwlbQxyLG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
            "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/12.9 MB 3.6 MB/s eta 0:00:04\n",
            "      --------------------------------------- 0.3/12.9 MB 4.0 MB/s eta 0:00:04\n",
            "     - -------------------------------------- 0.5/12.9 MB 4.1 MB/s eta 0:00:04\n",
            "     - -------------------------------------- 0.6/12.9 MB 3.7 MB/s eta 0:00:04\n",
            "     -- ------------------------------------- 0.9/12.9 MB 3.9 MB/s eta 0:00:04\n",
            "     ---- ----------------------------------- 1.3/12.9 MB 4.2 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.6/12.9 MB 4.4 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 1.8/12.9 MB 4.7 MB/s eta 0:00:03\n",
            "     ------ --------------------------------- 2.1/12.9 MB 4.5 MB/s eta 0:00:03\n",
            "     ------- -------------------------------- 2.4/12.9 MB 4.9 MB/s eta 0:00:03\n",
            "     -------- ------------------------------- 2.6/12.9 MB 4.7 MB/s eta 0:00:03\n",
            "     --------- ------------------------------ 3.0/12.9 MB 5.0 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.3/12.9 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.6/12.9 MB 5.3 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.7/12.9 MB 5.0 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 4.0/12.9 MB 5.1 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.2/12.9 MB 5.2 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.4/12.9 MB 5.0 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.6/12.9 MB 5.1 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 4.9/12.9 MB 5.2 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 5.2/12.9 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.7/12.9 MB 5.3 MB/s eta 0:00:02\n",
            "     ------------------ --------------------- 5.9/12.9 MB 5.3 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 6.3/12.9 MB 5.4 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 6.6/12.9 MB 5.5 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 6.8/12.9 MB 5.5 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 7.0/12.9 MB 5.4 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 7.4/12.9 MB 5.5 MB/s eta 0:00:02\n",
            "     ----------------------- ---------------- 7.7/12.9 MB 5.5 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 7.9/12.9 MB 5.5 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.4/12.9 MB 5.6 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.6/12.9 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.7/12.9 MB 5.5 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 9.3/12.9 MB 5.8 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.6/12.9 MB 5.8 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.9/12.9 MB 5.8 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.1/12.9 MB 5.8 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.2/12.9 MB 5.6 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 10.6/12.9 MB 5.9 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.1/12.9 MB 6.1 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.2/12.9 MB 6.1 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.5/12.9 MB 6.0 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.7/12.9 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.9 MB 6.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.4/12.9 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.7/12.9 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.9/12.9 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.9/12.9 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.9/12.9 MB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from es-core-news-sm==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.24.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.28.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\dayan\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dayan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YqLqIztqx0_u"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "#Este es para español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "#Este es "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HVtipgnY01q7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#para acceder a los datos del drive\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#para acceder a los datos del drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpBguJCl1Zzu"
      },
      "outputs": [],
      "source": [
        "doc_file = '/content/drive/MyDrive/3. PLN/MATERIAL CURSO OTROS/MATERIAL 2/data/corpus_mundo_today.csv'\n",
        "file_txt = open(doc_file, encoding=\"utf8\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwPs-weH1e3Q"
      },
      "outputs": [],
      "source": [
        "file_txt[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v78b76ou1sLc"
      },
      "source": [
        "Hacemos una estructura de control para separar los párrafos y eliminar los \"||\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOF2o8Uu0_bE"
      },
      "outputs": [],
      "source": [
        "doc_list = list()\n",
        "for line in file_txt.split('\\n'):\n",
        "    line = line.split('||')\n",
        "    doc_list.append(line[1] + ' ' + line[2])\n",
        "doc_list = doc_list[1:] # Elimino la cabecera del fichero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJOpwQxW11rP"
      },
      "outputs": [],
      "source": [
        "doc_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONJNMT1o0X7Q"
      },
      "source": [
        "## Algunos ejemplos con Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0F5PLlA90bNi"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'doc_list' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Tokenización doc: tokeniza en spacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m doc \u001b[39m=\u001b[39m nlp(doc_list[\u001b[39m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39m# imprimimos qué tipo de dato es:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTipo de dato: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(doc)))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'doc_list' is not defined"
          ]
        }
      ],
      "source": [
        "# Tokenización doc: tokeniza en spacy\n",
        "doc = nlp(doc_list[0])\n",
        "# imprimimos qué tipo de dato es:\n",
        "print('Tipo de dato: ' + str(type(doc)))\n",
        "# estructura de control para \n",
        "print([w for w in doc])\n",
        "print([w.text for w in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cnNaUY-f3J55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Fabián es Matemático.', 'Todos estamos en el salón de clae!', 'Pero tenemos hambre.']\n"
          ]
        }
      ],
      "source": [
        "# Segmentación, es decir , dividir en las cadenas de texto en frases o párrafos usando: puntos, adiración, interrogante, etc.\n",
        "# se usa \"sentencier\"\n",
        "from spacy.pipeline import Sentencizer\n",
        "sentencizer = Sentencizer()\n",
        "doc = nlp(\"Fabián es Matemático. Todos estamos en el salón de clae! Pero tenemos hambre.\")\n",
        "print([frase.text for frase in doc.sents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eSmlPZ1v3zln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atentos - atento\n",
            ": - :\n",
            "Todos - todo\n",
            "en - en\n",
            "el - el\n",
            "río - río\n",
            "estaban - estar\n",
            "pescando - pescar\n",
            "cuando - cuando\n",
            "de - de\n",
            "repente - repente\n",
            "vieron - ver\n",
            "un - uno\n",
            "oso - oso\n",
            "gigante - gigante\n",
            "que - que\n",
            "se - él\n",
            "comía - comer\n",
            "un - uno\n",
            "salmón - salmón\n"
          ]
        }
      ],
      "source": [
        "# Lematización\n",
        "doc = nlp(\"Atentos : Todos en el río estaban pescando cuando de repente vieron un oso gigante que se comía un salmón\")\n",
        "for palabra in doc:  \n",
        "    print(palabra.text + \" - \" + palabra.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwzjKzL94TDe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de stop words: 521\n",
            "Stop words: ['dice', 'sean', 'cuantos', 'de', 'aunque', 'haceis', 'algunas', 'usas', 'menudo', 'lado', 'algunos', 'buen', 'todavía', 'dijo', 'mencionó', 'éstos', 'aquéllos', 'encima', 'tampoco', 'consideró', 'alli', 'aquí', 'llegó', 'para', 'tambien', 'tenemos', 'consigues', 'quien', 'añadió', 'sabes', 'sé', 'comentó', 'modo', 'ellas', 'voy', 'sino', 'segunda', 'pasada', 'éstas', 'mayor', 'quién', 'fuimos', 'mí', 'hacen', 'cuales', 'ningunos', 'tarde', 'encuentra', 'mejor', 'ellos', 'pero', 'nosotras', 'según', 'vaya', 'estuvo', 'acuerdo', 'alguno', 'señaló', 'tanto', 'buenos', 'usais', 'atras', 'última', 'pueda', 'nada', 'otras', 'hacerlo', 'ocho', 'cuánta', 'podriamos', 'tan', 'aquellas', 'donde', 'mía', 'qué', 'tuya', 'e', 'solos', 'hizo', 'sea', 'entre', 'suyas', 'ya', 'siendo', 'mediante', 'solas', 'dicho', 'sabemos', 'nuestras', 'contra', 'sólo', 'suya', 'grandes', 'dejó', 'junto', 'saben', 'saber', 'adelante', 'aquellos', 'tienen', 'le', 'conseguimos', 'os', 'días', 'nunca', 'ir', 'estamos', 'tuyo', 'nos', 'míos', 'quizas', 'varios', 'tuvo', 'ni', 'usar', 'buenas', 'ser', 'considera', 'podrían', 'siempre', 'tercero', 'estas', 'yo', 'misma', 'realizar', 'ciertos', 'tuyas', 'tener', 'anterior', 'vuestra', 'queremos', 'ultimo', 'podría', 'luego', 'través', 'partir', 'más', 'poner', 'esto', 'aquel', 'usamos', 'alrededor', 'este', 'eran', 'sola', 'habrá', 'fui', 'dia', 'nuestros', 'nueva', 'medio', 'día', 'propia', 'cierto', 'creo', 'suyos', 'tercera', 'cuándo', 'once', 'aseguró', 'ese', 'cinco', 'te', 'total', 'próximos', 'o', 'llevar', 'mas', 'esos', 'ella', 'claro', 'debajo', 'veces', 'cuanta', 'él', 'poder', 'aqui', 'hemos', 'manera', 'delante', 'demasiado', 'tres', 'largo', 'estos', 'debe', 'podria', 'del', 'cuantas', 'fin', 'he', 'existen', 'todavia', 'cuanto', 'éste', 'hasta', 'pocos', 'dado', 'estais', 'proximo', 'tu', 'ciertas', 'mismas', 'ésa', 'informo', 'otro', 'tendrán', 'hay', 'les', 'vuestro', 'cada', 'haber', 'sabe', 'explicó', 'sí', 'cuando', 'dicen', 'mias', 'parte', 'da', 'ha', 'temprano', 'cuántos', 'fue', 'tiene', 'estaba', 'hubo', 'solamente', 'tenga', 'eras', 'hago', 'podriais', 'muchas', 'sido', 'eramos', 'excepto', 'quedó', 'tenía', 'pesar', 'próximo', 'ése', 'tengo', 'hace', 'algún', 'consigue', 'conseguir', 'verdadero', 'aquél', 'hablan', 'segun', 'siete', 'cuál', 'mal', 'enseguida', 'tuyos', 'sin', 'estan', 'afirmó', 'realizado', 'esa', 'vosotras', 'se', 'a', 'detrás', 'vamos', 'habia', 'nuevo', 'cuántas', 'grande', 'tus', 'porque', 'gran', 'primera', 'dar', 'estará', 'allí', 'repente', 'algo', 'paìs', 'habían', 'primer', 'manifestó', 'conocer', 'bien', 'muy', 'primero', 'contigo', 'nuevos', 'primeros', 'ambos', 'desde', 'mucha', 'pasado', 'ningunas', 'dias', 'embargo', 'despacio', 'informó', 'cual', 'mientras', 'usan', 'haciendo', 'mio', 'todas', 'hacia', 'los', 'pronto', 'va', 'eso', 'me', 'van', 'un', 'realizó', 'sus', 'dio', 'mios', 'otros', 'hacer', 'somos', 'antes', 'alguna', 'habla', 'que', 'respecto', 'soy', 'son', 'ahi', 'podrá', 'suyo', 'ésos', 'deben', 'como', 'ademas', 'ante', 'sois', 'ti', 'podrán', 'consiguen', 'después', 'tendrá', 'consigo', 'aproximadamente', 'haya', 'y', 'pocas', 'verdad', 'dentro', 'indicó', 'propios', 'toda', 'están', 'bajo', 'hacemos', 'nadie', 'siguiente', 'todos', 'cuatro', 'eres', 'otra', 'las', 'sera', 'propio', 'tú', 'trata', 'unas', 'parece', 'hoy', 'al', 'ésta', 'ahí', 'usa', 'aun', 'aquella', 'aquello', 'seis', 'agregó', 'cuánto', 'el', 'podrias', 'sigue', 'cómo', 'con', 'asi', 'dónde', 'diez', 'diferente', 'incluso', 'salvo', 'últimas', 'posible', 'ahora', 'pudo', 'así', 'dijeron', 'unos', 'mucho', 'menos', 'breve', 'haces', 'estados', 'últimos', 'ésas', 'quiere', 'solo', 'último', 'nosotros', 'quiza', 'su', 'sería', 'cuenta', 'esta', 'pues', 'teneis', 'estaban', 'mia', 'debido', 'quiénes', 'la', 'ver', 'mismos', 'lleva', 'muchos', 'uno', 'tras', 'cuáles', 'además', 'había', 'aquélla', 'mías', 'segundo', 'también', 'pueden', 'aún', 'por', 'quienes', 'podeis', 'si', 'mío', 'usted', 'demás', 'puede', 'arriba', 'poco', 'vuestros', 'quizás', 'mi', 'casi', 'fuera', 'mismo', 'qeu', 'dan', 'quizá', 'sabeis', 'todo', 'despues', 'peor', 'nuestra', 'existe', 'hicieron', 'podrian', 'era', 'nueve', 'u', 'igual', 'deprisa', 'es', 'esas', 'está', 'entonces', 'enfrente', 'supuesto', 'estar', 'detras', 'decir', 'en', 'ninguno', 'sobre', 'uso', 'doce', 'no', 'cierta', 'puedo', 'varias', 'vez', 'será', 'conmigo', 'aquéllas', 'dieron', 'nuevas', 'poca', 'podemos', 'buena', 'apenas', 'nuestro', 'tenido', 'bueno', 'fueron', 'lo', 'vais', 'vosotros', 'han', 'tal', 'final', 'vuestras', 'serán', 'mis', 'durante', 'cualquier', 'estado', 'ningún', 'dos', 'verdadera', 'expresó', 'diferentes', 'propias', 'bastante', 'ello', 'una', 'hecho', 'estoy', 'ustedes', 'ninguna']\n"
          ]
        }
      ],
      "source": [
        "# Stop Word o palabras de parada \n",
        "# No aportan al significado de la frase\n",
        "# hay 521 palabras en Español.\n",
        "# se usa : spacy.lang.es.stop_words.STOP_WORDS\n",
        "stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "print('Número de stop words: ' + str(len(stopwords)))\n",
        "print('Stop words: ' + str(list(stopwords)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "56cNwPVn48DX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atentos\n",
            ",\n",
            "río\n",
            "pescando\n",
            "vieron\n",
            "oso\n",
            "gigante\n",
            "comía\n",
            "salmón\n"
          ]
        }
      ],
      "source": [
        "# Los token en Spacy tienen un método denominado is_stop \n",
        "# es una variable lógica indicando si el token está o no en la lista \n",
        "doc = nlp(\"Atentos, Todos en el río estaban pescando cuando de repente vieron un oso gigante que se comía un salmón\")\n",
        "for palabra in doc:\n",
        "    if not palabra.is_stop:\n",
        "        print(palabra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pkedZFUN5OmT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>PoS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Atentos</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Todos</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>el</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>río</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>estaban</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>pescando</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cuando</td>\n",
              "      <td>SCONJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>de</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>repente</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>vieron</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>un</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>oso</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>gigante</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>que</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>se</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>comía</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>un</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>salmón</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text    PoS\n",
              "0    Atentos    ADJ\n",
              "1          ,  PUNCT\n",
              "2      Todos   PRON\n",
              "3         en    ADP\n",
              "4         el    DET\n",
              "5        río   NOUN\n",
              "6    estaban    AUX\n",
              "7   pescando   VERB\n",
              "8     cuando  SCONJ\n",
              "9         de    ADP\n",
              "10   repente   NOUN\n",
              "11    vieron   VERB\n",
              "12        un    DET\n",
              "13       oso   NOUN\n",
              "14   gigante   NOUN\n",
              "15       que   PRON\n",
              "16        se   PRON\n",
              "17     comía   VERB\n",
              "18        un    DET\n",
              "19    salmón   NOUN"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tagging : Part of Speech - Categorizar las palabras en un texto (corpus) o etiquetado gramatical. \n",
        "# pos: etiqueta de más alto nivel (verbo, nombre)\n",
        "\n",
        "doc = nlp(\"Atentos, Todos en el río estaban pescando cuando de repente vieron un oso gigante que se comía un salmón\")\n",
        "pos = [[tk.text, tk.pos_] for tk in doc]\n",
        "\n",
        "# los podemos organizar en un Dataframe\n",
        "import pandas as pd\n",
        "pd.DataFrame(pos, columns=[\"Text\", \"PoS\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oLTsvJz8LD1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "doc = nlp(\"Atentos, Todos en el río estaban pescando cuando de repente vieron un oso gigante que se comía un salmón\")\n",
        "parte_oracion = [palabra.pos_ for palabra in doc]\n",
        "Counter(parte_oracion).most_common()\n",
        "pd.DataFrame(Counter(parte_oracion).most_common(), columns=[\"Text\", \"PoS\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5v8ns-z9sWn"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "doc = nlp(doc_list[0])\n",
        "parte_oracion = [palabra.pos_ for palabra in doc]\n",
        "Counter(parte_oracion).most_common()\n",
        "pd.DataFrame(Counter(parte_oracion).most_common(), columns=[\"Text\", \"PoS\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rlyped8875R"
      },
      "source": [
        "Finalmente,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsFmrxH489NZ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "doc = nlp(\"Atentos, Todos en el río estaban pescando cuando de repente vieron un oso gigante que se comía 10 salmones\")\n",
        "\n",
        "resultado = [[tk.text, tk.lemma_, tk.pos_, tk.is_alpha, tk.is_stop] for tk in doc]\n",
        "pd.DataFrame(resultado, columns=[\"Text\", \"Lema\", \"PoS\", \"Alpha\", \"is Stop word\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q71OwJXm-GnG"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PmAdXm6-Dk1"
      },
      "outputs": [],
      "source": [
        "import es_core_news_md\n",
        "nlp_md = es_core_news_md.load()\n",
        "from spacy import displacy\n",
        "from IPython.display import SVG\n",
        "article_text = '''La ONG Fundación del Río explicó este Fabián viernes que la decisión de la Organización de la ONU para la Educación, la Ciencia y la Cultura (Unesco) de declarar como geoparque el río Coco, ubicado en el norte de Nicaragua, obliga a las autoridades nicaragüenses a proteger su ecosistema, ya que se encuentra en el área más deforestada de la cuenca. La Unesco está reconociendo la importancia del río Coco, pero también está haciendo un llamado al Gobierno a que actúe en la protección y la conservación de esos ecosistemas, dijo a Efe el presidente de la Fundación del Río, Amaru Ruiz.'''\n",
        "doc = nlp_md(article_text)\n",
        "SVG(data = displacy.render(doc, style=\"ent\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQFbWNt-_llW"
      },
      "outputs": [],
      "source": [
        "# similitud de palabras\n",
        "tokens = nlp_md(\"perro gato banana manzana rey reina\")\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B__gPBkxAUba"
      },
      "source": [
        "=============================================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C14IL8HARSC8"
      },
      "source": [
        "# Bag of Words (BoW) : Bolsa de Palabras\n",
        "\n",
        "- Es un modelo para generar como su nombre lo indica una bolsa de palabras que simplifique el contenido de documentos.\n",
        "\n",
        "- En los **BoW** se omiten la grámatica y el orden de las palabras.\n",
        "\n",
        "- Se concentra en la frecuencia de las palabras, es decir, en el número de veces que aparece en el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KskwjpufSRrI"
      },
      "outputs": [],
      "source": [
        "# Supongamos que tenemos un documento así:\n",
        "documento = [\"carlos juega mucho carlos carlos carro carro juega universidad universidad juega juega  mucho mucho carlos\",\n",
        "             \"conduce conduce conduce carro carro mucho mucho carro carro carro carro carro\",\n",
        "             \"universidad universidad MACC MACC MACC MACC MACC MACC UR UR UR UR UR UR UR\",\n",
        "             \"universidad universidad universidad carlos carlos conduce conduce carlos carlos carlos UR UR UR UR\",\n",
        "             \"universidad universidad carlos carlos carlos conduce conduce carro carro carro carro mucho mucho mucho\",\n",
        "             \"carlos juega universidad carro mucho juega mucho\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSJ7HDD7TreN"
      },
      "outputs": [],
      "source": [
        "# el método split() divide las cadenas de texto y obtenemos listas con cada oración (elemento de documento)\n",
        "for doc in documento:\n",
        "  print(doc.split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X9blhdrTOmG"
      },
      "outputs": [],
      "source": [
        "# construimos la bolsa de palabras\n",
        "bow = dict()\n",
        "for doc in documento:\n",
        "  doc = doc.split(\" \")\n",
        "  for palabra in doc:\n",
        "    if palabra in bow:\n",
        "      bow[palabra] +=1\n",
        "    else:\n",
        "      bow[palabra]=1\n",
        "print(bow)      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcqv0NOPT9r1"
      },
      "source": [
        "Observe que se genera un diccionario con la frecuencia de cada palabra. Sin embrago, cuando se construye una bolsa de palabras también es importante asignar un \"peso\" que determina cierto nivel de importancia en el documento o corpus.\n",
        "\n",
        "Existen varias formas de generar una bolsa de palabras.\n",
        "\n",
        "- Vectores de frecuencias\n",
        "\n",
        "- One Hot Encode\n",
        "\n",
        "- TF-IDF : Term Frequency-Inverse Document Frequency: Es una medida que pondera el uso de una determinada palabra dentro de un conjunto de documentos y que supone por lo tanto un elemento importante y relevante para la clasificación de documentos frente a la consulta de un usuario\n",
        "\n",
        "Para generar bolsas de palabras usaremos tres librerías diferentes:\n",
        "\n",
        "- **scikit**: aprendizaje automático\n",
        "\n",
        "- **NLTK** : procesamiento de lenguaje natural\n",
        "\n",
        "- **Gensim**: está específicamente diseñado para manejar grandes colecciones de texto, utilizando el flujo de datos y algoritmos incrementales eficientes. Se fundamenta en Numpy y Scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpY-Hqe_VCxP"
      },
      "source": [
        "## Vectores de Frecuencias\n",
        "\n",
        "- Es la forma más fácil de hacer una bolsa de palabras.\n",
        "\n",
        "- Consiste en contar cuantas palabras aparecen en el documento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy2b4iqjVQPo"
      },
      "outputs": [],
      "source": [
        "# Usando scikit tenemos : sklearn.feature_extraction.text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizar = CountVectorizer()\n",
        "vectores = vectorizar.fit_transform(documento)\n",
        "print(vectorizar.get_feature_names_out())\n",
        "print(vectores.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjAE4QvDVoBY"
      },
      "outputs": [],
      "source": [
        "# usando NLTK : from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download(\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWlEFSCqVzVs"
      },
      "outputs": [],
      "source": [
        "# tarea construir una función que cuente, ver este ejemplo con una entreda.\n",
        "tokens = nltk.word_tokenize(documento[0])\n",
        "Freq_dist=nltk.FreqDist(tokens) ; Freq_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdr_3NRNXDGl"
      },
      "outputs": [],
      "source": [
        "# Usando Gensim\n",
        "import gensim\n",
        "tokenizar = [nltk.word_tokenize(text) for text in documento]\n",
        "diccionario = gensim.corpora.Dictionary(tokenizar)\n",
        "vectores = [diccionario.doc2bow(token) for token in tokenizar]\n",
        "\n",
        "# Resultados\n",
        "print('\\nApariciones de las palabras en los documentos:')\n",
        "vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i065MwvUaeMx"
      },
      "source": [
        "## One Hot Encode\n",
        "\n",
        "- Para construir una *bolsa de palabras* con este método lo que hacemos es asignar 0 y 1 : equivalentes a False o True según si una palabra está o no en el documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP0lAanZbR4s"
      },
      "outputs": [],
      "source": [
        "# Usando scikit\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "vectorizar = CountVectorizer()\n",
        "corpus = vectorizar.fit_transform(documento)\n",
        "\n",
        "onehot = Binarizer()\n",
        "corpus = onehot.fit_transform(corpus.toarray())\n",
        "\n",
        "# Resultados\n",
        "print(vectorizar.get_feature_names_out())\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtcorUNcbxoU"
      },
      "outputs": [],
      "source": [
        "# usando NLTK\n",
        "# ejercicio hacer un diccionario key = palabra y el valor = true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RuY_RjdcHwG"
      },
      "outputs": [],
      "source": [
        "# Usando Gensim\n",
        "# la estructura es similar a la frecuencia de palabras, solo que ya no aparece la frecuencia sino uno.\n",
        "tokenize = [nltk.word_tokenize(text) for text in documento]\n",
        "dictionary = gensim.corpora.Dictionary(tokenize)\n",
        "vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in tokenize]\n",
        "\n",
        "# Resultados\n",
        "print('Diccionario de palabras -> palabra: id\\n')\n",
        "print(dictionary.token2id)\n",
        "print('\\nApariciones de las palabras en los documentos (id, 1):')\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H00xqXSrcfx1"
      },
      "source": [
        "## TF-IDF : Frecuencia de Término-Frecuencia Inversa de Documento\n",
        "\n",
        "Es una medida que pondera el uso de una determinada palabra dentro de un conjunto de documentos y que supone por lo tanto un elemento importante y relevante para la clasificación de documentos frente a la consulta de un usuario.\n",
        "\n",
        "- Construir la Bolsa de Palabras con TF-IDF en vez de con frecuencias evita dar \"importancia\" a texto muy largos y con mucha repetición de palabras, frente a textos cortos y con pocas repeticiones de palabras.\n",
        "\n",
        "\n",
        "- ***TF-IDF*** tiene dos componentes que son:\n",
        "    \n",
        "    * ***TF*** (Term Frecuency): Es la frecuencia con la que aparece la palabra en un documento del corpus. Esta se define como:\n",
        "    \n",
        "    $$tf(t,d) = 1 + log(f_{t,d})$$\n",
        "    \n",
        "    * ***IDF*** (Inverse Document Frequency): La frecuencia inversa del documento nos indica lo común que es una palabra en el corpus.\n",
        "    \n",
        "    $$idf(t,D) = log(1 + \\frac{N}{n_t})$$\n",
        "    \n",
        "\n",
        "* ***TF-IDF*** queda definido como:\n",
        "$$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)$$\n",
        "\n",
        "## Ejemplo\n",
        "\n",
        "* Supongamos un corpus de 2 documentos con las siguientes palabras:\n",
        "\n",
        "```\n",
        "corpus = [\"carro carro carro ur ur feliz\",\n",
        "          \"carro ur fabian fabian fabian\"]\n",
        "```\n",
        "\n",
        "* ***Ejemplo 1***: Calculamos el ***tf-idf*** de la palabra \"**carro**\" para el documento 1:\n",
        "    \n",
        "    * ***TF***:\n",
        "\n",
        "        - t: número de veces que aparece la palabra \"carro\" en el documento 1 es 3.\n",
        "        - d: número de palabras que tiene el documento 1 es 6\n",
        "\n",
        "        $$tf(t,d) = 1 + log(\\frac{3}{6}) =  0,69$$\n",
        "        \n",
        "    * ***IDF***:\n",
        "\n",
        "        - n<sub>t</sub>: número de documentos en los que aparece la palabra \"carro\" es 2.\n",
        "\n",
        "        - D: número total de documentos en el corpus es 2\n",
        "\n",
        "        $$idf(t,D) = log(1 + \\frac{2}{2}) = 0,3$$\n",
        "        \n",
        "    * ***TF-IDF***:\n",
        "    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,69 * 0,3 = 0,21$$\n",
        "\n",
        "\n",
        "* ***Ejemplo 2***: Calculamos el ***tf-idf*** de la palabra \"**fabian**\" para el documento 2:\n",
        "    \n",
        "    * ***TF***:\n",
        "        - t: número de veces que aparece la palabra \"fabian\" en el documento 2 es 3\n",
        "        - d: número de palabras que tiene el documento 1 es 5\n",
        "\n",
        "        $$tf(t,d) = 1 + log(\\frac{3}{5}) =  0,78$$\n",
        "        \n",
        "    * ***IDF***:\n",
        "\n",
        "        - n<sub>t</sub>: número de documentos en los que aparece la palabra 'fabian' es 1\n",
        "        - D: número total de documentos en el corpus es 2\n",
        "\n",
        "        $$idf(t,D) = log(1 + \\frac{2}{1}) = 0,48$$\n",
        "        \n",
        "    * ***TF-IDF***:\n",
        "\n",
        "    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,78 * 0,48 = 0,37$$\n",
        "\n",
        "\n",
        "**Observación:** \n",
        "\n",
        "- Las implementaciones del ***TF-IDF*** de **Scikit** y **Gensim** estan pensadas para corpus con un número relevante de documentos y de palabras, por tanto la implementación del ***TF-IDF*** acepta una serie de parámetros para no tener en cuenta Stop Words, palabras irrelevantes, etc. \n",
        "\n",
        "- Si se realiza una implementación del ***TF-IDF*** no van a conincidir los resultados de esa implementación con los resultados de las librerías de **Scikit** y **Gensim** a no ser que se modifiquen los parámetros de las funciones del ***TF-IDF***.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgRs5SuginSN"
      },
      "outputs": [],
      "source": [
        "# en Scikit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(documento)\n",
        "\n",
        "# Resultados\n",
        "print(tfidf.get_feature_names_out ())\n",
        "print(corpus.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN2-sp7FjT4q"
      },
      "outputs": [],
      "source": [
        "# en Gensim\n",
        "tokenize = [nltk.word_tokenize(text) for text in documento]\n",
        "dictionary = gensim.corpora.Dictionary(tokenize)\n",
        "tfidf = gensim.models.TfidfModel(dictionary=dictionary, normalize=True)\n",
        "vectors = [tfidf[dictionary.doc2bow(doc)] for doc in tokenize]\n",
        "\n",
        "# Resultados\n",
        "print('Diccionario de palabras -> palabra: id\\n')\n",
        "print(dictionary.token2id)\n",
        "print('\\nApariciones de las palabras en los documentos (id, tfidf):')\n",
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkuwrCz0pVUg"
      },
      "source": [
        "## ETIQUETADO MORFOLÓGICO : (*part-of-speech tagging*)\n",
        "\n",
        "NLTK nos da algunas herramientas para crear etiquetadores morfológicos, es decir, identificar si una palabra es verbo, sustantivo, adjetivo , etc.\n",
        "\n",
        "CC : coodinate conjunction\n",
        "\n",
        "CD : cardinal number\n",
        "\n",
        "DT : determiner\n",
        "\n",
        "EX ; existencial there\n",
        "\n",
        "FW : foeign word\n",
        "\n",
        "IN : preprosition or subordinating\n",
        "\n",
        "JJ : adjetive\n",
        "\n",
        "JJR : adjetive, comparative\n",
        "\n",
        "JJS : adjetive superlative\n",
        "...\n",
        "BV : verbo forma base\n",
        "\n",
        "VBD : verbo pasado\n",
        "\n",
        "VBG : verb gerundio\n",
        "\n",
        "ver\n",
        "\n",
        "[TAGGING](https://pythonspot.com/nltk-speech-tagging/)\n",
        "\n",
        "Usaremos primer : nltk.pos_tag que es un etiquetador morfológico basado en aprendizaje automático. A partir de miles de ejemplos de oraciones etiquetadas manualmente, el sistema *ha aprendido*, calculando frecuencias y generalizando cuál es la categoría gramatical más probable para cada token. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APvBZtBBpj3q"
      },
      "outputs": [],
      "source": [
        "# inicialmente usaremos una función de NLTK denominada : nltk.pos_tag\n",
        "\n",
        "oracion1 = \"This is the lost dog I found at the park\".split()\n",
        "oracion2 = \"The progress of the humankind as I progress\".split()\n",
        "oracion3 = \"When I went to the University, I played soccer\".split()\n",
        "print(nltk.pos_tag(oracion1))\n",
        "print(nltk.pos_tag(oracion2))\n",
        "print(nltk.pos_tag(oracion3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwo-Apxp70r"
      },
      "source": [
        "### Etiquetador basado en Expresiones Regulares\n",
        "\n",
        "- Las expresiones regulares nos permiten especificar cadenas de texto.\n",
        "\n",
        "- Esta nos permite identificar patrones en las palabras para poderlas clasificar\n",
        "\n",
        "- Vamos a crear nuestra propia lista para etiquetar. (tagging)\n",
        "\n",
        "- El primer elemento es la expresión regular y la segunda la categoría gramatical. \n",
        "\n",
        "- Usamos la instrucción `RegexpTagger`. \n",
        "\n",
        "- Este ejemplo lo haremos en inglés\n",
        "\n",
        "Usaremos Raw String Notation (r'texto') : una forma en la cual Python nos permite implementar expresiones regulares.\n",
        "\n",
        "- El método que usamos es **.tag**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "529kEnWZqD_v"
      },
      "outputs": [],
      "source": [
        "¿Cuál es el error?\n",
        "pattern = [(r'(March)$', \"TAG\")] # $ : final de palabra\n",
        "tagger=nltk.RegexpTagger(pattern)\n",
        "\n",
        "print(tagger.tag('He was born in March 1991'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpJLE4JaqPuX"
      },
      "outputs": [],
      "source": [
        "patrones = [\n",
        "    (r'[Aa]m$', 'VBP'),               # irregular forms of 'to be' \n",
        "    (r'[Aa]re$', 'VBP'),              #  \n",
        "    (r'[Ii]s$', 'VBZ'),               #  \n",
        "    (r'[Ww]as$', 'VBD'),              #  \n",
        "    (r'[Ww]ere$', 'VBD'),             #  \n",
        "    (r'[Bb]een$', 'VBN'),             #  \n",
        "\n",
        "    (r'[Hh]ave$', 'VBP'),             # irregular forms of 'to be' \n",
        "    (r'[Hh]as$', 'VBZ'),              #  \n",
        "    (r'[Hh]ad$', 'VBD'),              #\n",
        "\n",
        "    (r'^I$', 'PRP'),                  # personal pronouns\n",
        "    (r'[Yy]ou$', 'PRP'),              # \n",
        "    (r'[Hh]e$', 'PRP'),               # \n",
        "    (r'[Ss]he$', 'PRP'),              # \n",
        "    (r'[Ii]t$', 'PRP'),               # \n",
        "    (r'[Tt]hey$', 'PRP'),             # \n",
        "    (r'[Aa]n?$', 'AT'),               #  (? se repite el caracter una o cero vez)\n",
        "    (r'[Tt]he$', 'AT'),               # \n",
        "\n",
        "    (r'[wW]h.+$', 'WP'),              # wh- pronoun  (. es cualquier caracter y + que se repite una o más veces)\n",
        "\n",
        "    (r'.*ing$', 'VBG'),               # gerunds\n",
        "\n",
        "    (r'.*ed$', 'VBD'),                # simple past\n",
        "\n",
        "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
        "\n",
        "    (r'[Cc]an(not|n\\'t)?$', 'MD'),    # modals\n",
        "    (r'[Mm]ight$', 'MD'),             # \n",
        "    (r'[Mm]ay$', 'MD'),               # \n",
        "\n",
        "    (r'.+ould$', 'MD'),               # modals: could, should, would\n",
        "\n",
        "    (r'.*ly$', 'RB'),                 # adverbs (.* re repite un caracter cero o más veces)\n",
        "\n",
        "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
        "\n",
        "    (r'.*s$', 'NNS'),                 # plural nouns\n",
        "\n",
        "    (r'-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
        "\n",
        "    (r'^to$', 'TO'),                  # to \n",
        "\n",
        "    (r'^in$', 'IN'),                  # in prep\n",
        "\n",
        "    (r'^[A-Z]+([a-z])*$', 'NNP'),     # proper nouns \n",
        "\n",
        "    (r'.*', 'NN')                     # nouns (default)\n",
        "]\n",
        "\n",
        "regexTagger = nltk.RegexpTagger(patrones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxTCiqNkqXXu"
      },
      "outputs": [],
      "source": [
        "print(regexTagger.tag(\"I was taking a sunbath in Alpedrete\".split()))\n",
        "\n",
        "print(regexTagger.tag(\"She would have found 100 dollars in the bag\".split()))\n",
        "\n",
        "print(regexTagger.tag(\"DSFdfdsfsd 1852 to dgdfgould fXXXdg in XXXfdg\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMMMrNM9q1Ja"
      },
      "source": [
        "# Taller de normalización\n",
        "\n",
        "* Se comparte los siguientes datos (./data/corpus_mundo_today.csv). fuente: \"https://www.elmundotoday.com/ (artículo)\n",
        "\n",
        "\n",
        "* Este CSV esta formado por 3 campos que son:\n",
        "\n",
        "    - Tema\n",
        "    - Título\n",
        "    - Texto\n",
        "    \n",
        "* **Objetivo**: Normalizar el ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas.\n",
        "\n",
        "## Punto 1\n",
        "\n",
        "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
        "\n",
        "1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "\n",
        "\n",
        "2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "\n",
        "        - **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        - **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "\n",
        "3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "        - **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        - **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "\n",
        "4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "        - **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        - **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "     \n",
        "5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "        -  **input**: lista de documentos (lista de Strings).\n",
        "        - **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "\n",
        "## Punto 2 \n",
        "\n",
        "Aprovechando la normalización realizada anteriormente se pide:\n",
        "\n",
        "6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "\n",
        "7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
        "\n",
        "\n",
        "8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brVX0MnHsuHy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
